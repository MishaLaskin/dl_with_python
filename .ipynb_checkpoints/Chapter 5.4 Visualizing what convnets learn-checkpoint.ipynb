{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5.3: Transfer Learning\n",
    "\n",
    "The general thought here is that if you train a ConvNet to classify a large set of classes from a large pool of data (e.g. millions training examples / thousands of classes), then the ConvNet develops a generic set of representations from all the feature maps it learned. \n",
    "\n",
    "These representations (edges, shapes, etc) can then be uses to classify things outside of the initial classes\n",
    "\n",
    "## Feature extraction\n",
    "\n",
    "Use the trained ConvNet layers and only replace the class-specific Dense layers at the end of the net. Here are all the feature extraction pre-trained nets maintained in Keras\n",
    "\n",
    "- Xception \n",
    "- Inception V3 \n",
    "- ResNet50\n",
    "- VGG16\n",
    "- VGG19\n",
    "- MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras import layers \n",
    "from keras import models\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "# weights - specifies where to get pretrained weights from\n",
    "# include_top - include or not densely connected \"top\" layer\n",
    "# input_shape - what to expect from incoming data, if not specified network will take anything\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# shows structure of VGG16 net\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The two paths of feature extraction learning\n",
    "\n",
    "### Option 1: FAST FEATURE EXTRACTION WITHOUT DATA AUGMENTATION\n",
    "\n",
    "Running the convolutional base over your dataset, recording its output to a Numpy array on disk, and then using this data as input to a standalone, densely connected classifier \n",
    "\n",
    "- pros: fast and cheap, because you only run conv base once for every image\n",
    "- cons: you can't augment data with this method\n",
    "\n",
    "\n",
    "### Option 2: FEATURE EXTRACTION WITH DATA AUGMENTATION\n",
    "\n",
    "Extending the conv base model by adding Dense layers on top and running whole thing end to end on training data\n",
    "\n",
    "- pros: you can augment data\n",
    "- cons: expensive because now you're rerunning the algorithm\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "sample count 2000\n",
      "i * batch_size 20\n",
      "sample count 2000\n",
      "i * batch_size 40\n",
      "sample count 2000\n",
      "i * batch_size 60\n",
      "sample count 2000\n",
      "i * batch_size 80\n",
      "sample count 2000\n",
      "i * batch_size 100\n",
      "sample count 2000\n",
      "i * batch_size 120\n",
      "sample count 2000\n",
      "i * batch_size 140\n",
      "sample count 2000\n",
      "i * batch_size 160\n",
      "sample count 2000\n",
      "i * batch_size 180\n",
      "sample count 2000\n",
      "i * batch_size 200\n",
      "sample count 2000\n",
      "i * batch_size 220\n",
      "sample count 2000\n",
      "i * batch_size 240\n",
      "sample count 2000\n",
      "i * batch_size 260\n",
      "sample count 2000\n",
      "i * batch_size 280\n",
      "sample count 2000\n",
      "i * batch_size 300\n",
      "sample count 2000\n",
      "i * batch_size 320\n",
      "sample count 2000\n",
      "i * batch_size 340\n",
      "sample count 2000\n",
      "i * batch_size 360\n",
      "sample count 2000\n",
      "i * batch_size 380\n",
      "sample count 2000\n",
      "i * batch_size 400\n",
      "sample count 2000\n",
      "i * batch_size 420\n",
      "sample count 2000\n",
      "i * batch_size 440\n",
      "sample count 2000\n",
      "i * batch_size 460\n",
      "sample count 2000\n",
      "i * batch_size 480\n",
      "sample count 2000\n",
      "i * batch_size 500\n",
      "sample count 2000\n",
      "i * batch_size 520\n",
      "sample count 2000\n",
      "i * batch_size 540\n",
      "sample count 2000\n",
      "i * batch_size 560\n",
      "sample count 2000\n",
      "i * batch_size 580\n",
      "sample count 2000\n",
      "i * batch_size 600\n",
      "sample count 2000\n",
      "i * batch_size 620\n",
      "sample count 2000\n",
      "i * batch_size 640\n",
      "sample count 2000\n",
      "i * batch_size 660\n",
      "sample count 2000\n",
      "i * batch_size 680\n",
      "sample count 2000\n",
      "i * batch_size 700\n",
      "sample count 2000\n",
      "i * batch_size 720\n",
      "sample count 2000\n",
      "i * batch_size 740\n",
      "sample count 2000\n",
      "i * batch_size 760\n",
      "sample count 2000\n",
      "i * batch_size 780\n",
      "sample count 2000\n",
      "i * batch_size 800\n",
      "sample count 2000\n",
      "i * batch_size 820\n",
      "sample count 2000\n",
      "i * batch_size 840\n",
      "sample count 2000\n",
      "i * batch_size 860\n",
      "sample count 2000\n",
      "i * batch_size 880\n",
      "sample count 2000\n",
      "i * batch_size 900\n",
      "sample count 2000\n",
      "i * batch_size 920\n",
      "sample count 2000\n",
      "i * batch_size 940\n",
      "sample count 2000\n",
      "i * batch_size 960\n",
      "sample count 2000\n",
      "i * batch_size 980\n",
      "sample count 2000\n",
      "i * batch_size 1000\n",
      "sample count 2000\n",
      "i * batch_size 1020\n",
      "sample count 2000\n",
      "i * batch_size 1040\n",
      "sample count 2000\n",
      "i * batch_size 1060\n",
      "sample count 2000\n",
      "i * batch_size 1080\n",
      "sample count 2000\n",
      "i * batch_size 1100\n",
      "sample count 2000\n",
      "i * batch_size 1120\n",
      "sample count 2000\n",
      "i * batch_size 1140\n",
      "sample count 2000\n",
      "i * batch_size 1160\n",
      "sample count 2000\n",
      "i * batch_size 1180\n",
      "sample count 2000\n",
      "i * batch_size 1200\n",
      "sample count 2000\n",
      "i * batch_size 1220\n",
      "sample count 2000\n",
      "i * batch_size 1240\n",
      "sample count 2000\n",
      "i * batch_size 1260\n",
      "sample count 2000\n",
      "i * batch_size 1280\n",
      "sample count 2000\n",
      "i * batch_size 1300\n",
      "sample count 2000\n",
      "i * batch_size 1320\n",
      "sample count 2000\n",
      "i * batch_size 1340\n",
      "sample count 2000\n",
      "i * batch_size 1360\n",
      "sample count 2000\n",
      "i * batch_size 1380\n",
      "sample count 2000\n",
      "i * batch_size 1400\n",
      "sample count 2000\n",
      "i * batch_size 1420\n",
      "sample count 2000\n",
      "i * batch_size 1440\n",
      "sample count 2000\n",
      "i * batch_size 1460\n",
      "sample count 2000\n",
      "i * batch_size 1480\n",
      "sample count 2000\n",
      "i * batch_size 1500\n",
      "sample count 2000\n",
      "i * batch_size 1520\n",
      "sample count 2000\n",
      "i * batch_size 1540\n",
      "sample count 2000\n",
      "i * batch_size 1560\n",
      "sample count 2000\n",
      "i * batch_size 1580\n",
      "sample count 2000\n",
      "i * batch_size 1600\n",
      "sample count 2000\n",
      "i * batch_size 1620\n",
      "sample count 2000\n",
      "i * batch_size 1640\n",
      "sample count 2000\n",
      "i * batch_size 1660\n",
      "sample count 2000\n",
      "i * batch_size 1680\n",
      "sample count 2000\n",
      "i * batch_size 1700\n",
      "sample count 2000\n",
      "i * batch_size 1720\n",
      "sample count 2000\n",
      "i * batch_size 1740\n",
      "sample count 2000\n",
      "i * batch_size 1760\n",
      "sample count 2000\n",
      "i * batch_size 1780\n",
      "sample count 2000\n",
      "i * batch_size 1800\n",
      "sample count 2000\n",
      "i * batch_size 1820\n",
      "sample count 2000\n",
      "i * batch_size 1840\n",
      "sample count 2000\n",
      "i * batch_size 1860\n",
      "sample count 2000\n",
      "i * batch_size 1880\n",
      "sample count 2000\n",
      "i * batch_size 1900\n",
      "sample count 2000\n",
      "i * batch_size 1920\n",
      "sample count 2000\n",
      "i * batch_size 1940\n",
      "sample count 2000\n",
      "i * batch_size 1960\n",
      "sample count 2000\n",
      "i * batch_size 1980\n",
      "sample count 2000\n",
      "i * batch_size 2000\n",
      "Found 1000 images belonging to 2 classes.\n",
      "sample count 1000\n",
      "i * batch_size 20\n",
      "sample count 1000\n",
      "i * batch_size 40\n",
      "sample count 1000\n",
      "i * batch_size 60\n",
      "sample count 1000\n",
      "i * batch_size 80\n",
      "sample count 1000\n",
      "i * batch_size 100\n",
      "sample count 1000\n",
      "i * batch_size 120\n",
      "sample count 1000\n",
      "i * batch_size 140\n",
      "sample count 1000\n",
      "i * batch_size 160\n",
      "sample count 1000\n",
      "i * batch_size 180\n",
      "sample count 1000\n",
      "i * batch_size 200\n",
      "sample count 1000\n",
      "i * batch_size 220\n",
      "sample count 1000\n",
      "i * batch_size 240\n",
      "sample count 1000\n",
      "i * batch_size 260\n",
      "sample count 1000\n",
      "i * batch_size 280\n",
      "sample count 1000\n",
      "i * batch_size 300\n",
      "sample count 1000\n",
      "i * batch_size 320\n",
      "sample count 1000\n",
      "i * batch_size 340\n",
      "sample count 1000\n",
      "i * batch_size 360\n",
      "sample count 1000\n",
      "i * batch_size 380\n",
      "sample count 1000\n",
      "i * batch_size 400\n",
      "sample count 1000\n",
      "i * batch_size 420\n",
      "sample count 1000\n",
      "i * batch_size 440\n",
      "sample count 1000\n",
      "i * batch_size 460\n",
      "sample count 1000\n",
      "i * batch_size 480\n",
      "sample count 1000\n",
      "i * batch_size 500\n",
      "sample count 1000\n",
      "i * batch_size 520\n",
      "sample count 1000\n",
      "i * batch_size 540\n",
      "sample count 1000\n",
      "i * batch_size 560\n",
      "sample count 1000\n",
      "i * batch_size 580\n",
      "sample count 1000\n",
      "i * batch_size 600\n",
      "sample count 1000\n",
      "i * batch_size 620\n",
      "sample count 1000\n",
      "i * batch_size 640\n",
      "sample count 1000\n",
      "i * batch_size 660\n",
      "sample count 1000\n",
      "i * batch_size 680\n",
      "sample count 1000\n",
      "i * batch_size 700\n",
      "sample count 1000\n",
      "i * batch_size 720\n",
      "sample count 1000\n",
      "i * batch_size 740\n",
      "sample count 1000\n",
      "i * batch_size 760\n",
      "sample count 1000\n",
      "i * batch_size 780\n",
      "sample count 1000\n",
      "i * batch_size 800\n",
      "sample count 1000\n",
      "i * batch_size 820\n",
      "sample count 1000\n",
      "i * batch_size 840\n",
      "sample count 1000\n",
      "i * batch_size 860\n",
      "sample count 1000\n",
      "i * batch_size 880\n",
      "sample count 1000\n",
      "i * batch_size 900\n",
      "sample count 1000\n",
      "i * batch_size 920\n",
      "sample count 1000\n",
      "i * batch_size 940\n",
      "sample count 1000\n",
      "i * batch_size 960\n",
      "sample count 1000\n",
      "i * batch_size 980\n",
      "sample count 1000\n",
      "i * batch_size 1000\n",
      "Found 1000 images belonging to 2 classes.\n",
      "sample count 1000\n",
      "i * batch_size 20\n",
      "sample count 1000\n",
      "i * batch_size 40\n",
      "sample count 1000\n",
      "i * batch_size 60\n",
      "sample count 1000\n",
      "i * batch_size 80\n",
      "sample count 1000\n",
      "i * batch_size 100\n",
      "sample count 1000\n",
      "i * batch_size 120\n",
      "sample count 1000\n",
      "i * batch_size 140\n",
      "sample count 1000\n",
      "i * batch_size 160\n",
      "sample count 1000\n",
      "i * batch_size 180\n",
      "sample count 1000\n",
      "i * batch_size 200\n",
      "sample count 1000\n",
      "i * batch_size 220\n",
      "sample count 1000\n",
      "i * batch_size 240\n",
      "sample count 1000\n",
      "i * batch_size 260\n",
      "sample count 1000\n",
      "i * batch_size 280\n",
      "sample count 1000\n",
      "i * batch_size 300\n",
      "sample count 1000\n",
      "i * batch_size 320\n",
      "sample count 1000\n",
      "i * batch_size 340\n",
      "sample count 1000\n",
      "i * batch_size 360\n",
      "sample count 1000\n",
      "i * batch_size 380\n",
      "sample count 1000\n",
      "i * batch_size 400\n",
      "sample count 1000\n",
      "i * batch_size 420\n",
      "sample count 1000\n",
      "i * batch_size 440\n",
      "sample count 1000\n",
      "i * batch_size 460\n",
      "sample count 1000\n",
      "i * batch_size 480\n",
      "sample count 1000\n",
      "i * batch_size 500\n",
      "sample count 1000\n",
      "i * batch_size 520\n",
      "sample count 1000\n",
      "i * batch_size 540\n",
      "sample count 1000\n",
      "i * batch_size 560\n",
      "sample count 1000\n",
      "i * batch_size 580\n",
      "sample count 1000\n",
      "i * batch_size 600\n",
      "sample count 1000\n",
      "i * batch_size 620\n",
      "sample count 1000\n",
      "i * batch_size 640\n",
      "sample count 1000\n",
      "i * batch_size 660\n",
      "sample count 1000\n",
      "i * batch_size 680\n",
      "sample count 1000\n",
      "i * batch_size 700\n",
      "sample count 1000\n",
      "i * batch_size 720\n",
      "sample count 1000\n",
      "i * batch_size 740\n",
      "sample count 1000\n",
      "i * batch_size 760\n",
      "sample count 1000\n",
      "i * batch_size 780\n",
      "sample count 1000\n",
      "i * batch_size 800\n",
      "sample count 1000\n",
      "i * batch_size 820\n",
      "sample count 1000\n",
      "i * batch_size 840\n",
      "sample count 1000\n",
      "i * batch_size 860\n",
      "sample count 1000\n",
      "i * batch_size 880\n",
      "sample count 1000\n",
      "i * batch_size 900\n",
      "sample count 1000\n",
      "i * batch_size 920\n",
      "sample count 1000\n",
      "i * batch_size 940\n",
      "sample count 1000\n",
      "i * batch_size 960\n",
      "sample count 1000\n",
      "i * batch_size 980\n",
      "sample count 1000\n",
      "i * batch_size 1000\n"
     ]
    }
   ],
   "source": [
    "# FAST FEATURE EXTRACTION WITHOUT DATA AUGMENTATION\n",
    "\n",
    "# Extracting features using the pretrained convolutional base\n",
    "\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#if os.path.exists(\"./cats_and_dogs_small\"):\n",
    "#    shutil.rmtree('./cats_and_dogs_small')\n",
    "\n",
    "    \n",
    "base_dir = './cats_and_dogs_small'\n",
    "train_dir = os.path.join(base_dir, 'train') \n",
    "validation_dir = os.path.join(base_dir, 'validation') \n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "datagen = ImageDataGenerator(rescale=1./255) \n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512)) \n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(directory, \n",
    "                                            target_size=(150, 150), \n",
    "                                            batch_size=batch_size, \n",
    "                                            class_mode='binary')\n",
    "    i=0\n",
    "    \n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        print('sample count',sample_count)\n",
    "        print('i * batch_size',i * batch_size)\n",
    "\n",
    "        # break after every image was seen once\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000) \n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000) \n",
    "test_features, test_labels = extract_features(test_dir, 1000)\n",
    "\n",
    "\n",
    "# So far, we used conv_base to extract features from our data using a pre-trained model\n",
    "# we still don't have a head, so our current model output is a (# samples, 4, 4, 512) tensor\n",
    "\n",
    "# flatten the features\n",
    "\n",
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))\n",
    "\n",
    "# viola, the data is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 724us/step - loss: 0.6042 - acc: 0.6695 - val_loss: 0.4321 - val_acc: 0.8390\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 674us/step - loss: 0.4201 - acc: 0.8090 - val_loss: 0.3548 - val_acc: 0.8770\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 2s 867us/step - loss: 0.3560 - acc: 0.8500 - val_loss: 0.3193 - val_acc: 0.8860\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 628us/step - loss: 0.3055 - acc: 0.8730 - val_loss: 0.2954 - val_acc: 0.8790\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 649us/step - loss: 0.2861 - acc: 0.8750 - val_loss: 0.2840 - val_acc: 0.8880\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 648us/step - loss: 0.2520 - acc: 0.9060 - val_loss: 0.2740 - val_acc: 0.8950\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 686us/step - loss: 0.2483 - acc: 0.8960 - val_loss: 0.2619 - val_acc: 0.8980\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 2s 887us/step - loss: 0.2347 - acc: 0.9090 - val_loss: 0.2652 - val_acc: 0.8880\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 653us/step - loss: 0.2153 - acc: 0.9140 - val_loss: 0.2755 - val_acc: 0.8730\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 678us/step - loss: 0.1998 - acc: 0.9250 - val_loss: 0.2579 - val_acc: 0.8920\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 655us/step - loss: 0.1905 - acc: 0.9295 - val_loss: 0.2502 - val_acc: 0.8980\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 2s 826us/step - loss: 0.1821 - acc: 0.9335 - val_loss: 0.2455 - val_acc: 0.9030\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 738us/step - loss: 0.1755 - acc: 0.9345 - val_loss: 0.2398 - val_acc: 0.9030\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 644us/step - loss: 0.1662 - acc: 0.9345 - val_loss: 0.2380 - val_acc: 0.9030\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 643us/step - loss: 0.1562 - acc: 0.9485 - val_loss: 0.2373 - val_acc: 0.9030\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 654us/step - loss: 0.1509 - acc: 0.9460 - val_loss: 0.2368 - val_acc: 0.9060\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 2s 863us/step - loss: 0.1419 - acc: 0.9540 - val_loss: 0.2446 - val_acc: 0.8960\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 728us/step - loss: 0.1397 - acc: 0.9525 - val_loss: 0.2343 - val_acc: 0.9080\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 668us/step - loss: 0.1331 - acc: 0.9555 - val_loss: 0.2484 - val_acc: 0.8930\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 659us/step - loss: 0.1237 - acc: 0.9545 - val_loss: 0.2505 - val_acc: 0.8920\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 693us/step - loss: 0.1213 - acc: 0.9565 - val_loss: 0.2342 - val_acc: 0.9090\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 2s 885us/step - loss: 0.1170 - acc: 0.9615 - val_loss: 0.2334 - val_acc: 0.9040\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 672us/step - loss: 0.1091 - acc: 0.9675 - val_loss: 0.2350 - val_acc: 0.9060\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 649us/step - loss: 0.1060 - acc: 0.9645 - val_loss: 0.2374 - val_acc: 0.9020\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 636us/step - loss: 0.1026 - acc: 0.9675 - val_loss: 0.2402 - val_acc: 0.8960\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 732us/step - loss: 0.0987 - acc: 0.9715 - val_loss: 0.2526 - val_acc: 0.8910\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 2s 828us/step - loss: 0.0975 - acc: 0.9660 - val_loss: 0.2365 - val_acc: 0.9050\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 646us/step - loss: 0.0920 - acc: 0.9705 - val_loss: 0.2389 - val_acc: 0.9000\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 671us/step - loss: 0.0862 - acc: 0.9735 - val_loss: 0.2371 - val_acc: 0.8980\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 649us/step - loss: 0.0850 - acc: 0.9715 - val_loss: 0.2386 - val_acc: 0.8980\n"
     ]
    }
   ],
   "source": [
    "# now define and train the top of the convnet: the Dense layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6IuIZBAFfRGRDPL/AZq/WH0dfpxkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5928fc048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5a8d62160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: FEATURE EXTRACTION WITH DATA AUGMENTATION\n",
    "\n",
    "In the previous example, we got decent results but overfit because we didn't augment the data. Let's try the classification again, this time using option 2. \n",
    "\n",
    "We'll create an augmented data set and then pass it through the conv_base as well as our own dense layer on top of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 16,812,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Adding a densely connected classifier on top of the convolutional base\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEFORE TRAINING, FREEZE CONVOLUTIONAL BASE\n",
    "\n",
    "This is important because you don't want to change the weights of the mode. If you don't freeze, all the features learned by the pre-trained model will be tossed. That would such and defeat the whole point of the exercise. If you don't freeze, give up and go home.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable weight tensors before freezing the conv base: 30\n",
      "This is the number of trainable weight tensors after freezing the conv base: 4\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "print('This is the number of trainable weight tensors '\n",
    "         'before freezing the conv base:', len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print('This is the number of trainable weight tensors '\n",
    "          'after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "  4/100 [>.............................] - ETA: 13:51 - loss: 0.5760 - acc: 0.7375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f17ebf9fae6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       validation_steps=50)\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model end to end with a frozen convolutional base\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "#Note that the validation data shouldnt be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "\n",
    "# going to take VERY LONG to train, ~15 mins per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning: another method for transfer learning\n",
    "\n",
    "Unfreeze top layers of a frozen model, and jointly train the newly added part of the model and the top layers. \n",
    "\n",
    "The point is to adjust the more abstract representation to fit the new classification task. Pretty cool and intuitive.\n",
    "\n",
    "1. Add your custom network on top of an already-trained base network.\n",
    "2. Freeze the base network.\n",
    "3. Train the part you added.\n",
    "4. Unfreeze some layers in the base network.\n",
    "5. Jointly train both these layers and the part you added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fine tune the last layer, so freeze blocks 1-4 and train block 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Freezing all layers up to a specific one\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 57/100 [================>.............] - ETA: 2:10 - loss: 0.4057 - acc: 0.8114"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=1,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)\n",
    "\n",
    "# Time per epoch is ~ 5 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "\n",
    "- Convnets are the best type of machine-learning models for computer-vision tasks. Its possible to train one from scratch even on a very small dataset, with decent results.\n",
    "- On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when youre working with image data.\n",
    "- Its easy to reuse an existing convnet on a new dataset via feature extraction. This is a valuable technique for working with small image datasets.\n",
    "- As a complement to feature extraction, you can use fine-tuning, which adapts to a new problem some of the representations previously learned by an existing model. This pushes performance a bit further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
