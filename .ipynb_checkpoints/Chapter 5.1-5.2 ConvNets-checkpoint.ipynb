{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Conv Nets\n",
    "\n",
    "Start with simple MNIST example\n",
    "\n",
    "Conv nets take shapes of \n",
    "\n",
    "```python\n",
    "(image_height, image_width, image_channels)\n",
    "```\n",
    "\n",
    "does not including the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras import layers \n",
    "from keras import models\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Flatten 3D tensor into 1D\n",
    "model.add(layers.Flatten())\n",
    "# Reshape\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# Classify\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 28s 470us/step - loss: 0.1725 - acc: 0.9456\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 28s 469us/step - loss: 0.0469 - acc: 0.9856\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 28s 467us/step - loss: 0.0326 - acc: 0.9897\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 28s 468us/step - loss: 0.0245 - acc: 0.9924\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 28s 469us/step - loss: 0.0202 - acc: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff89bb57278>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# reshape the data to what network expects\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "# normalize\n",
    "train_images = train_images.astype('float32') / 255\n",
    "# reshape\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "# normalize\n",
    "test_images = test_images.astype('float32') / 255\n",
    "# vectorize\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 167us/step\n",
      "Test score: 0.0331606782337\n",
      "Test accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(test_images, test_labels)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! CNNs are really powerful. We got >99% accuracy on the test data. Holy mackerel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Nets explained\n",
    "\n",
    "### Convolutions\n",
    "\n",
    "A convolution works by \n",
    "\n",
    "Step 1) sliding a window (usually 3x3) over the 3D input feature map, stopping at every location, and extracting the 3D patch of surrounding features.\n",
    "\n",
    "Step 2) Transform 3D patch of output features into 1D vector by doing a tensor product between the 3D patch and the weights of the layer (convolution kernel).\n",
    "\n",
    "Step 3) Reassemble all of these 1D vectors into 3D output map.\n",
    "\n",
    "Note: Typically the output map has the same spatial dimensions as input with some exceptions.\n",
    "\n",
    "Conv nets take a 3D tensor (height, width, channel) where the channel is the pixel value (scalar if it's black and white, and a 3 unit vector if it's RGB). \n",
    "\n",
    "A convolution takes a 3D tensor, extracts patches from it and applies a transformation to all of those patches.\n",
    "\n",
    "The first convolution inputs this 3D tensors and outputs a 3D tensor. The new tensor also has a height and width as well as a channel. However, the channel is no longer the RGB or grayscale pixel value, it's now a filter. Filters store info about discovered features (e.g. edges, circles, etc) and the new width / height image show you how prevalent this feature is in the image. \n",
    "\n",
    "1. Filters (or features) are stored in the channel axis\n",
    "2. The 2D image stored for each filter shows where a filter occurs in the image\n",
    "\n",
    "![Image](https://s3-us-west-2.amazonaws.com/mishalaskin/dl/convnetfilter.png)\n",
    "\n",
    "\n",
    "Convolutions are defined by two key parameters:\n",
    "\n",
    "a. Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5. In the example, they were 3 × 3, which is a common choice.\n",
    "\n",
    "b. Depth of the output feature map—The number of filters computed by the convolution. The example started with a depth of 32 and ended with a depth of 64.\n",
    "\n",
    "In keras, you implement one with\n",
    "\n",
    "```python\n",
    "Conv2D(number_of_filters, (patch_height, patch_width)).\n",
    "```\n",
    "\n",
    "#### Strides\n",
    "\n",
    "#### Padding\n",
    "\n",
    "### MaxPooling\n",
    "\n",
    "MaxPooling reduces the image size by scanning the image with a (typically 2X2 with stride 2) patch and taking the maximum value from the 2X2 matrix. This is done for 2 reasons:\n",
    "\n",
    "1. Compressing the image allows a convolution filter to learn abstract features\n",
    "2. it dramatically reduces the number of parameters in the model\n",
    "\n",
    "# Practice problem: classify image with single object using ConvNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
