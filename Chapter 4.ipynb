{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple hold out validation\n",
    "\n",
    "Split training data into training / validation. That's all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://s3-us-west-2.amazonaws.com/mishalaskin/dl/simpleholdoutvalidation.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "Image(url= \"https://s3-us-west-2.amazonaws.com/mishalaskin/dl/simpleholdoutvalidation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K fold validation\n",
    "\n",
    "Split data into $K$ partitions. For each partition $i$, train model on remaining $K-1$ partitions and evaluate on partition $i$ \n",
    "\n",
    "To boost this, you can run K validation multiple times, and shuffle the data after you finish a validation batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://s3-us-west-2.amazonaws.com/mishalaskin/dl/kfoldvalidation.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://s3-us-west-2.amazonaws.com/mishalaskin/dl/kfoldvalidation.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "Taking data and turning into a tensor to feed the neural network. One-hot encoding is a form of vectorization\n",
    "\n",
    "## Value normalization\n",
    "\n",
    "Values need to be normalized to [0,1] interval\n",
    "\n",
    "For learning to be easy for NN, data should\n",
    "\n",
    "1. Take small values - typically most values should be in 0-1 range\n",
    "2. Be homogenous - features should take values in similar range\n",
    "\n",
    "Also, use the following normalization convention\n",
    "\n",
    "3. Normalize each feature independently to have a mean of 0.\n",
    "4. Normalize each feature independently to have a standard deviation of \n",
    "\n",
    "## Missing values\n",
    "\n",
    "For missing values enter them as 0, you definitely need to train on them so that the network learns these values are not useful.\n",
    "\n",
    "## Feature engineering\n",
    "\n",
    "Pruning data to make learning easier. For example, if your data is a collections of stock price images, training on the image itself is computationally expensive. Instead you can extract the coordinates of the points on the graph and use those as the input data.\n",
    "\n",
    "Basically just use common sense when preparing input data.\n",
    "\n",
    "## Overfitting / Underfitting\n",
    "\n",
    "If you can, get more training data. More data beats everything.\n",
    "\n",
    "If not, try regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization techniques\n",
    "\n",
    "## 1. Reducing network size\n",
    "\n",
    "Capacity = number of parameters in network. Large networks have high capacity, as you take Capacity => infinity you can approximate any nonlinear function.\n",
    "\n",
    "But this also means you quickly overfit the training data. Taking parameters out of the model makes it simpler, and less likely to overfit.\n",
    "\n",
    "## 2. Weight regularization\n",
    "\n",
    "Limits the values weights can take. By limiting weights to smaller values, you simplify the model which makes it less likely to be dominated by outlier values.\n",
    "\n",
    "There are two common types of weight regularization\n",
    "\n",
    "1. L1 reg - $cost \\propto |W|$\n",
    "2. L2 reg - $cost \\propto |W|^2$\n",
    "\n",
    "Here is an example of adding reg in keras\n",
    "\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n",
    "\n",
    "the value 0.001 is the decay coefficient, the larger it is the stronger the regularization. Regularization is added during training, so training loss will be higher than test loss.\n",
    "\n",
    "Here are reg types you can add in Keras\n",
    "\n",
    "L1:\n",
    "regularizers.l1(0.001)\n",
    "\n",
    "L2:\n",
    "regularizers.l2(0.001)\n",
    "\n",
    "L1 and L2:\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)\n",
    "\n",
    "## 3. Dropout\n",
    "\n",
    "Randomly drop out output features of in the layers. If a handful of features are artificially dominating the network, drop out combats this bias by taking them out at random.\n",
    "\n",
    "![Image](https://s3-us-west-2.amazonaws.com/mishalaskin/dl/dropout.png)\n",
    "\n",
    "\n",
    "In Keras dropout is simple. Here's an example\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "The 0.5 coefficient is the rate of drop out (i.e. dropout 50% of features)\n",
    "\n",
    "# Final thoughts\n",
    "\n",
    "## Choosing last layer activation function\n",
    "\n",
    "![Image](https://s3-us-west-2.amazonaws.com/mishalaskin/dl/lastlayeractivation.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
